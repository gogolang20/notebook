第五章 服务:让客户端发现**pod**并与之通信

### 5.1 介绍服务

Kubemetes service 是一种为一组功能相同的 pod 提供单一不变的接入点的资源

5.1.1 创建服务

```
在前面的章节中,通过创建 ReplicationController 运行了三个包含 Node.js 应用的 pod。
再次创建 ReplicationController 并且确认 pod 启动运行,
在这之后将会为这三个 pod 创建一个服务。
-------------------------------------------------------------
kubectl create -f kubia-rc.yaml #4.2.2示例
-------------------------------------------------------------

创建服务的最简单的⽅法是通过kubectl expose.

vim /root/kubia-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  ports:
  - port: 80 #该服务的可用端口
    targetPort: 8080 #服务将连接转发到的容器端口
  selector:  #具有 app=kubia 标签的pod都属于该服务
    app: kubia
kubectl create -f kubia-svc.yaml
$ service/kubia created
#它将在端口80接收请求并将连接路由到具有标签选择器是app=kubia的pod的8080端⼜上

kubectl get svc #查看服务
kubectl get pod

如何在已有的pod中运⾏命令
kubectl exec {podName} --curl -s http://Cluster-IP #运行的容器中远程执行命令
eg:kubectl exec kubia-bjchh -- curl -s http://10.103.204.242 
$ You've hit kubia-ptk57
#双横杠(--)代表着kubectl命令项的结束。在两个横杠之后的内容是指在pod内部需要执⾏的命令。

配置服务上的会话亲和性
apiVersion: v1 
kind: Service 
spec: 
  sessionAffinity: ClientIP #表示亲和性
 ...
#Kubernetes仅仅支持两种形式的会话亲和性服务:None和ClientIP。

------------------
同⼀个服务暴露多个端⼜:
在创建一个有多个瑞口的服务的时候,必须给每个瑞口指定名字

在服务定义中指定多端口
apiVersion: v1 
kind: Service 
metadata:
  name: kubia
spec:
  ports:
  - name: http
    port: 80
    targetPort: 8080 #pod的8080端口映射成80端口
  - name: https
    port: 443
    targetPort: 8443
  selector:
    app: kubia #标签选择器适用于整个服务
#标签选择器应⽤于整个服务,不能对每个端⼜做单独的配置。如果不同的pod有不同的端⼜映射关系,需要创建两个服务。
kubectl create -f kubia-svc.yaml #修改kubia-svc.yaml再次部署
$ service/kubia created

eg:kubectl exec kubia-bjchh -- curl -s http://10.111.228.190
$ You've hit kubia-ptk57
```

```
!---------------------!
practice:
之前创建的 kubia po 不在多个端口上侦昕,因此可以练习创建 个多端口服务和 个多端口 pod
!---------------------!

在pod的定义中指定port名称 ！！！
kind:Pod
spec:
  containers:
  - name: kubia
    ports:
    - name: http
      containerPort: 8080 #端口8080被命名为http
    - name: https
      containerPort: 8443
可以在服务spec中按名称引⽤这些端⼜,如下⾯的代码清单所⽰
在服务中引⽤命名pod
apiVersion: v1 
kind: Service
spec:
  ports:
  - name: http
    port: 80
    targetPort: http #将端口80映射到容器中被称为http的端口
  - name: https
    port: 443
    targetPort: https

为什么要采⽤命名端口的⽅式？最⼤的好处就是即使更换端口号 也⽆须更改服务spec。你的pod现在对http服务⽤的是8080,但是假设过段时间你决定将端口更换为80呢？
如果你采⽤了命名的端口,仅仅需要做的就是改变spec pod 中的端口号(当然你的端口号的名称没有改变)。在你的pod向新端⼜更新时,根据pod收到的连接(8080端口在旧的pod上、80端口在新的pod上),⽤户连接将会转发到对应的端口号上。
```

5.1.2 服务发现

```
客户端pod如何知道服务的IP和端口？
是否需要先创建服务,然后⼿动查找其IP地址并将IP传递给客户端pod的配置选项？ 不需要！
1 通过环境变量发现服务
在pod开始运⾏的时候,Kubernetes会初始化⼀系列的环境变量指 向现在存在的服务.
如果你创建的服务早于客户端pod的创建,pod上的进程可以根据环境变量获得服务的IP地址和端口号。

先删除所有pod, 使得 ReplicationController 创建全新的 pod
kubectl delete po --all
#rc会自动创建新的pod

kubectl exec {podName} env
eg:kubectl exec kubia-qtzj8 env
$KUBIA_SERVICE_HOST= #服务的集群IP
$KUBIA_SERVICE_PORT= #服务所在的端口


2 通过 DNS 发现服务
Kubemetes 通过修改每个容器的 /etc/resolv.conf文件实现
pod 是否使用内部的 DNS 服务器是根据 pod中spec的dnsPolicy 属性来决定的

3 通过FQDN连接服务
kubectl exec -it {podName} -- /bin/bash
eg:kubectl exec -it kubia-c9wk6 -- /bin/bash
curl http://kubia.default.svc.cluster.local #pod容器内部执行,需要svc是开启的
curl http://kubia.default
curl http://kubia
$ You've hit kubia-c9wk6
#在请求的URL中,可以将服务的名称作为主机名来访问服务
cat /etc/resolv.conf #查看容器中的文件内容
ping kubia #无法ping通
#这是因为服务的集群IP是⼀个虚拟IP,并且只有在与服务端⼜结合时才有意义。
```

### 5.2 连接集群外部的服务

5.2.1 介绍服务 endpoint

```
kubectl describe svc kubia #查看endpoint
kubectl get endpoints kubia
kubectl get pod -o wide #IP与上面的显示一致
```

5.2.2 手动配置服务的 endpoint

```
不含pod选择器的服务
vim /root/external-service.yaml
apiVersion: v1 
kind: Service
metadata:
  name: external-service #服务的名字必须和Endpoint对象的名字相匹配
spec:
  ports:
  - port: 80
kubectl create -f external-service.yaml
$ service/external-service created

kubectl describe svc external-service #没有信息
kubectl get endpoints external-service #没有信息

为没有选择器的服务创建Endpoint资源
手动创建Endpoint资源
vim /root/external-service-endpoints.yaml
apiVersion: v1 
kind: Endpoints
metadata:
  name: external-service #Endpoint的名称必须和服务的名称相匹配
subsets:
  - addresses:
    - ip: 11.11.11.11 #服务将连接重定向到endpoint的IP地址
    - ip: 22.22.22.22 #服务将连接重定向到endpoint的IP地址
    ports:
    - port: 80 #Endpoint的目标端口
kubectl create -f external-service-endpoints.yaml
$ endpoints/external-service created

kubectl describe svc external-service #更新信息
kubectl get endpoints external-service #更新信息
#Endpoint对象需要与服务具有相同的名称,并包含该服务的⽬标IP地址和端口列表。
```

5.2.3 为外部服务创建别名

```
除了⼿动配置服务的Endpoint来代替公开外部服务⽅法,有⼀种更简单的⽅法,就是通过其完全限定域名(FQDN)访问外部服务
ExternalName类型的服务
vim /root/external-service-extemalname.yaml
apiVersion: v1 
kind: Service
metadata:
  name: external-service
spec:
  type: ExternalName #代码的type被设置成ExternalName
  externalName: someapi.somecompany.com #实际服务的完全限定域名
  ports:
  - port: 80
kubectl create -f external-service-extemalname.yaml #先删除之前的
$ service/external-service created
#服务创建完成后,pod可以通过external-service.default.svc.cluster.local域名(甚⾄是external-service)连接到外部服务,⽽不是使⽤服务的实际FQDN。
#ExternalName 服务仅在DNS级别实施--为服务创建了简单的CNAME DNS记录。
注意 CNAME记录指向完全限定的域名⽽不是数字IP地址。
```

### 5.3 将服务暴露给外部客户端

```
有几种方式可以在外部访问服务
1 将服务的类型设置成NodePort
2 将服务的类型设置成LoadBalance,NodePort类型的一 种扩展
3 创建一个Ingress资源,这是一个完全不同的机制,通过一个IP地址公开多个服务 #HTTP层
```

5.3.1 使用 NodePort 类型的服务

```
创建NodePort类型的服务
vim /root/kubia-svc-nodeport.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-nodeport 
spec:
  type: NodePort #为NodePort设置服务类型
  ports:
  - port: 80 #服务集群IP的端口号
    targetPort: 8080 #背后pod的目标端口号
    nodePort: 30123 #通过集群节点的30123端口可以访问该服务
  selector:
    app: kubia
kubectl create -f kubia-svc-nodeport.yaml
$ service/kubia-nodeport created

kubectl get svc kubia-nodeport
curl CLUSTER-IP:80 #EXTERNAL-IP <none>,没有显示<nodee>
curl 192.168.152.150:30123
$ You've hit kubia-sjjpl
curl 192.168.152.151:30123
$ You've hit kubia-qtzj8

使⽤JSONPath获取所有节点的IP
#忽略 kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address'
#参考 http://kubernetes.io/docs/user-guide/jsonpath 文档
#GKE演示可能与kubeadm安装演示不同 ！！！
```

5.3.2 通过负载均衡器将服务暴露出来

```
Load Badancer类型的服务
vim /root/kubia-svc-loadbalancer.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
kubectl create -f kubia-svc-loadbalancer.yaml
$ service/kubia-loadbalancer created

kubectl get svc kubia-loadbalancer #查看服务
$ kubia-loadbalancer   LoadBalancer   10.103.3.207     <pending>                 80:30008/TCP     19s

curl 192.168.152.151:30008 #节点IP+port可以访问
$ You've hit kubia-c9wk6
curl 192.168.152.150:30008
$ You've hit kubia-qtzj8

会话亲和性和Web浏览器
浏览器使⽤keep-alive连接,并通过单个连接发送所有请求,⽽curl每次都会打开⼀个新连接。
```

5.3.3 了解外部连接的特性

```
了解并防⽌不必要的⽹络跳数
可以通过将服务配置为仅将外部通信重定向到接收连接的节点上 运⾏的pod来阻⽌此额外跳数。这是通过在服务的spec部分中设置externalTrafficPolicy字段来完成的 #不建议使用

记住客户端IP是不记录的
当集群内的客户端连接到服务时,⽀持服务的pod可以获取 客户端的IP地址。但是,当通过节点端⼜接收到连接时,由于对数据包执⾏了源⽹络地址转换(SNAT),因此数据包的源IP将发⽣更改。
```

### 5.4 通过Ingress暴露服务

```
为什么需要Ingress？
⼀个重要的原因是每个LoadBalancer 服务都需要⾃⼰的负载均衡器,以及独有的公有IP地址,⽽Ingress只需要⼀个公⽹IP就能为许多服务提供访问。

Ingress控制器是必不可少的
kubectl get po --all-namespaces #没用提供Ingress控制器pod
```

5.4.1 创建 Ingress 资源

```
vim /root/kubia-ingress.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
kubectl create -f kubia-ingress.yaml
$ ingress.extensions/kubia created
```

5.4.2 通过 Ingress 访问服务

```
kubectl get ingress #ADDRESS未显示IP

vim /etc/hosts #添加映射关系
显示的ip kubia.example.com 

curl http://kubia.example.com
```

5.4.3 通过相同的 ingress 暴露多个服务

```
将不同的服务映射到相同主机的不同路径
 ...
   - host: kubia.example.com
    http:
      paths:
      - path: /kubia
        backend:
          serviceName: kubia
          servicePort: 80
      - path: /foo
        backend:
          serviceName: bar
          servicePort: 80

将不同的服务映射到不同的主机上
Ingress根据不同的主机(host)暴露出多种服务
spec:
  rules:
  - host: foo.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: foo
          servicePort: 80
  - host: bar.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: bar
          servicePort: 80
```

5.4.4 配置Ingress处理TLS传输

```
为Ingress创建TLS认证
需要将证书和私钥 附加到Ingress,⾸先,需要创建私钥和证书
openssl genrsa -out tls.key 2048
openssl req -new -x509 -key tls.key -out tls.cert -days 360 -subj /CN=kubia.example.com
创建Secret
kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key
$ secret/tls-secret created
#官网参考文档:https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/
```

```
通过CertificateSigningRequest(CSR)资源签署证书

vim /root/kubia-ingress-tls.yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  tls:
  - hosts: 
    - kubia.example.com
    secretName: tls-secret
  rules:
  - host: kubia.example.com
    http:
      paths:
      - path: /
        backend:
          serviceName: kubia-nodeport
          servicePort: 80
kubectl apply -f kubia-ingress-tls.yaml #不需要删除即可部署
$ ingress.extensions/kubia configured

使⽤HTTPS通过Ingress访问服务
curl -k -v https://kubia.example.com/kubia #缺少ingress控制器组件

注意 对Ingress功能的⽀持因不同的Ingress控制器实现⽽异,因此请检查特定实现的⽂档以确定⽀持的内容。
```

### 5.5 pod 就绪后发出信号

5.5. 1 介绍就绪探针

```
就绪探针的类型
1 Exec 探针,执⾏进程的地⽅。容器的状态由进程的退出状态代码确定。
2 HTTPGET 探针,向容器发送HTTP GET请求,通过响应的HTTP状态代码判断容器是否准备好。
3 TCP socket 探针,它打开⼀个TCP连接到容器的指定端⼜。如果连接已建⽴,则认为容器已准备就绪。
```

5.5.2 向pod添加就绪探针

```
kubectl edit rc kubia
$ replicationcontroller/kubia edited
 ...
spec:
  replicas: 3
  selector:
    app: kubia
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: kubia
    spec:
      containers:
      - image: gogolang/kubia
        imagePullPolicy: Always
        name: kubia
        readinessProbe: #探针部分
          exec:
            command:
            - ls
            - /var/ready
删除pod后让rc重新创建
kubectl get po #查看新创建的pod名称
kubectl exec {podName} -- touch /var/ready
eg:kubectl exec kubia-j44nn -- touch /var/ready

kubectl get po -o wide #获取IP
curl 10.244.2.25:8080 (pod-IP)
curl 192.168.152.151:30008 (即使这个节点没有存活的pod)/ curl 192.168.152.150:30008 (node-IP) 
kubectl logs kubia-j44nn
$ Kubia server starting ... 
$  Received request from::ffff:10.244.0.0

kubectl describe po kubia-j44nn
$ Readiness:      exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3

务必定义就绪探针
不要将停⽌pod的逻辑纳⼊就绪探针中
```

### 5.6 使用 headless服务来发现独立的pod

客户端需要链接到所有的pod

通常, 当执⾏服务的DNS查找时,DNS服务器会返回单个IP——服务的集群IP。但是,如果告诉Kubernetes,不需要为服务提供集群IP(通过在服 务 spec中将clusterIP字段设置为None来完成此操作),则DNS服务器将返回pod IP⽽不是单个服务IP。

5.6.1 创建headless服务

```
vim /root/kubia-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia-headless
spec:
  clusterIP: None
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: kubia
kubectl create -f kubia-svc-headless.yaml
$ service/kubia-headless created
kubectl get svc kubia-headless #没有集群IP
kubectl describe svc kubia-headless #没有集群IP

kubectl exec {podName} -- touch /var/ready
eg:kubectl exec kubia-s8648 -- touch /var/ready
```

5.6.2 通过DNS发现pod
kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1 --command -- sleep infinity
$ pod/dnsutils created

kubectl get po -o wide
使⽤新创建的pod执⾏DNS查找
kubectl exec dnsutils -- nslookup kubia-headless
$ Server:        10.96.0.10
$ Address:    10.96.0.10#53

$ Name:    kubia-headless.default.svc.cluster.local
$ Address: 10.244.2.25
$ Name:    kubia-headless.default.svc.cluster.local
$ Address: 10.244.1.19

#注意 headless服务仍然提供跨pod的负载平衡,但是通过DNS轮询机制不是通过服务代理


5.6.3 发现所有的pod一包括未就绪的pod

要告诉Kubemetes无论pod的准备状态如何,希望将所有pod 添加到服务中。必须将以下注解添加到服务中:
kind: Service
metadata: 
  annotations:
    serviece.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
#具体还需要查看一下文档 ################


5.7 排除服务故障

如果⽆法通过服务访问pod,应该根据下⾯的列表进⾏排查
1 ⾸先,确保从集群内连接到服务的集群IP,⽽不是从外部。
2 不要通过ping服务IP来判断服务是否可访问(请记住,服务的集群IP是虚拟IP,是⽆法ping通的)。
3 如果已经定义了就绪探针,请确保它返回成功；否则该pod不会成为服务的⼀部分。
4 要确认某个容器是服务的⼀部分,请使⽤kubectl get endpoints来检查相应的端点对象。
5 如果尝试通过 FQDN 或其中一部分 访问服务(例如,myservice.mynamespace.svc.cluster.local或myservice.mynamespace),但并不起作⽤,请查看是否可以使⽤其集群 IP⽽不是FQDN来访问服务。
6 检查是否连接到服务公开的端口,⽽不是⽬标端口。
7 尝试直接连接到pod IP以确认pod正在接收正确端⼜上的连接。
8 如果甚至无法通过pod的IP访问应用,请确保应用不是仅绑定到本地主机。
