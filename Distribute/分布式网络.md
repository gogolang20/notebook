# 一 高并发负载均衡

## 网络协议原理

7层OSI网络模型

```shell
cd /proc/$$/fd 
pwd  
ll  #可以查看到输入输出文件描述符

#建立连接后,才会有有效数据的传输
exec 8<> /dev/tcp/www.baidu.com/80  
echo -e "GET / HTTP/1.0\n" >& 8   
exec 8<& -(关闭)

exec 8<> /dev/tcp/www.baidu.com/80  
echo -e "GET / HTTP/1.0\n" >& 8
cat 0<& 8	#请求回百度主页

#三次握手:建立面向链接的可靠传输协议；四次挥手！！！

netstat -natp   #IP地址显示方式 all tcp PID
netstat -tulpn  
```

```
vim /etc/sysconfig/network-scripts/ifcfg-ens33

IPADDR:IP地址,三类IP地址,最后一位数字是主机位

NETMASk(掩码):和IP地址做按位与运算 得到网络号；判定成功后走对应的网关出去

GATEWAY:网关

DNS:域名解析
```

下一跳机制:路由器做路由判定下一跳的地址,然后转发

```shell
route -n 可以查看路由条目
ping:ICMP协议
ping www.baidu.com

arp -a #查看网卡的MAC地址
```



## LVS的DR TUN NAT模型推导

首次通信之前 ARP目标MAC地址:FFFFFF 进行广播

```shell
ifconfig ens33:8  192.168.152.166/24 #添加一张网卡
route add -host 192.168.88.88 gw 192.168.152.13 #添加路由条目
route del -host 192.168.88.88 #删除路由条目
主机需要精确匹配,添加主机不需要给出网络位
```

高并发的IP问题(通信),客户端与服务器之前需要添加一层做负载均衡

四层负载均衡:接收固定Port的数据包,所以是四层

先用LVS(4层,不握手,但是查看包的端口号)负载均衡服务器 hold流量(后端服务器是镜像的)

后面用 nginx 反向代理服务器握手(7层 官方大约5万个连接)



### NAT网络模型(网络地址转换)

S-NAT(源地址转换协议):局域网连接公网

路由器封存自己不同的端口号区分发送出去的 局域网内每台主机IP:port

D-NAT(目标地址转换协议):负载均衡服务器



NAT网络模型瓶颈:

1 带宽瓶颈,非对称,请求的数据包比较小,回复的包比较大

2 来回的地址转化消耗算力

 

### DR网络模型

直接路由模型(ARP协议修改)

真实服务器添加一个隐藏的对外不可见,对内可见的负载均衡服务器IP地址；真实服务器信息不通过负载均衡服务器返回,而是直接返回给客户端,解决算力消耗与带宽问题

但是需要考虑换MAC地址才能让真实服务器收到包,同时负载均衡服务器与真实服务器必须在同一局域网内。

负载均衡服务器将收到的MAC地址替换成真实服务器RIP的MAC地址,转发的IP不改变,由于服务器端设置了隐藏的负载均衡服务器IP地址,服务器端可以直接接收处理

优点:速度快,成本低

缺点:不可以跨网络,动的是二层链路层MAC地址



### TUN 模型

解决物理空间位置问题,数据包背着数据包走

在真实数据数据包外在封一层IP地址

应用场景:VPN



## LVS的DR模型试验搭建

负载均衡是将并发分治,实际没有提升带宽

网络层IP:DIP(负载均衡分发IP) RIP(服务器真实IP) VIP(负载均衡虚拟IP)



ARP文件的网卡配置不可以使用vim编辑器修改,需要通过echo命令修改

arp_ignor 参数默认是0:定义接收到ARP请求的**响应级别**

arp_announce参数默认是0:定义将自己地址向外通告时的**通告级别**

计算机可以有多块网卡,修改网卡配置 MAC地址对外隐藏,对内可见

Linux系统一般有两张网卡 lo(loopback,虚拟的网卡)和 ens33(物理网卡),如何配置达到需要的效果？

- lo接口与外界时不连通的(适合配置VIP)



### 负载均衡调度算法

四种静态 :rr 轮询；wrr；dh；sh

动态调度算法:lc最少连接；wlc；sed；nq；LBLC；DH；LBLCR

负载均衡服务器如何知道真实服务器与客户端的连接数？这样才能调度

负载均衡服务器记录握手/分手包



### LVS搭建

```shell
LVS机器分布
node1	192.168.152.128		LVS节点
node2	192.168.152.133		从节点
node3	192.168.152.134		从节点

LVS是Linux中的ipvs内核模块
yum install -y ipvsadm
```



```shell
yum install -y net-tools
LVS节点:
ifconfig  ens33:2 192.168.152.100/24 
ifconfig  ens33:2 down #可以关闭,配置错误时使用

从节点node2
cd /proc/sys/net/ipv4/conf/ens33
echo 1 > arp_ignore 
echo 2 > arp_announce
cd /proc/sys/net/ipv4/conf/all
echo 1 > arp_ignore 
echo 2 > arp_announce
ifconfig lo:2 192.168.152.100 netmask 255.255.255.255 #添加VIP
从节点node3
echo 1 > /proc/sys/net/ipv4/conf/ens33/arp_ignore 
echo 2 > /proc/sys/net/ipv4/conf/ens33/arp_announce
echo 1 > /proc/sys/net/ipv4/conf/all/arp_ignore 
echo 2 > /proc/sys/net/ipv4/conf/all/arp_announce
ifconfig lo:2 192.168.152.100 netmask 255.255.255.255 #注意掩码 包会优先进lo网卡

从节点node2和node3
yum install -y httpd
systemctl start httpd #启动httpd服务
vi /var/www/html/index.html
	<h1> from 192.168.152.13X  </h1>
宿主机浏览器访问 192.168.152.13X 可以查看内容

LVS节点:
yum install -y ipvsadm #安装软件
ipvsadm -A -t 192.168.152.100:80  -s rr #配置入口包规则
ipvsadm -ln #查看配置的入口规则
ipvsadm -a -t 192.168.152.100:80  -r 192.168.152.133  -g -w 1
ipvsadm -a -t 192.168.152.100:80  -r 192.168.152.134  -g -w 1
ipvsadm -ln

http://192.168.152.100/ #刷新可以访问到负载的不同主页内容
netstat -natp # 看不到socket连接
ipvsadm -lnc #可以查看负载记录

FIN_WAIT #链接过,有记录包
SYN_RECV #LVS没有问题,证明后面网络层出问题了

从节点node2和node3
netstat -natp #可以查看socket连接

不是持久化的设置,重启后需要重新设置

ipvsadm --save > /etc/sysconfig/ipvsadm
```

```shell
ipvsadm -A -t 192.168.245.100:80 -s rr 
ipvsadm -a  -t 192.168.245.100:80 -r 192.168.245.133  -g  -w 1 
ipvsadm -a  -t 192.168.245.100:80 -r 192.168.245.134  -g  -w 1 
-A 进包规则
-a 负载规则
-t TCP协议
-s rr 调度方式 轮询
-g 轮询
-w 1 权重
192.168.245.100:80 分发IP:端口
192.168.245.133 真实服务器地址
192.168.245.134 真实服务器地址
```



## 基于keepalived的LVS高可用搭建

nginx:基于反向代理的 负载均衡,先握手再负载,服务返回也是要过nginx



LVS负载均衡器可能会挂,单点故障

真实服务器也可能挂,一部分用户会请求异常,LVS还存有这个真实服务器的负载记录,所以还会继续分发数据

如何解决？

LVS单点故障:一变多

- 主备(HA高可用)
- 主主:借助DNS实现

主备模式如何确定主节点是否正常

- 备用机主动询问健康检查
- 主节点主动通告

何如确定真实服务器挂了:访问一下,ping只能检测到网络层



```shell
keepalived实验:

LVS机器分布
node1	192.168.152.128		LVS节点
node2	192.168.152.133		从节点
node3	192.168.152.134		从节点
node4	192.168.152.135		LVS备用节点

LVS节点
ipvsadm -C #清楚ipvsadm配置
ifconfig  ens33:2 down #清除IP配置

从节点没有关机就不用再配置

LVS节点 和 LVS备用节点 安装keepalived ipvsadm
yum install -y keepalived
#配置文件
cd /etc/keepalived/
cp keepalived.conf keepalived.conf.bak #备份配置文件
vim keepalived.conf
	配置文件中 vrrp 虚拟路由冗余协议
yum install -y man

man 5 keepalived.conf #查看man 帮助文档
persistence_granularity <NETMASK>
persistence_granularity 255.255.255.0

d + G #删除光标位置后面的内容
shift: #选择光标
.,$-1y #从光标位置向下复制到倒数第二行

scp ./keepalived.conf  root@192.168.152.135:`pwd` #远程拷贝到LVS备用节点 pwd使用的反引号

删除原配置文件以下部分
   vrrp_skip_check_adv_addr
   vrrp_strict
   vrrp_garp_interval 0
   vrrp_gna_interval 0
```



```shell
LVS节点 keepalived.conf 配置文件

global_defs {
   notification_email {
     acassen@firewall.loc
     failover@firewall.loc
     sysadmin@firewall.loc
   }
   notification_email_from Alexandre.Cassen@firewall.loc
   smtp_server 192.168.200.1
   smtp_connect_timeout 30
   router_id LVS_DEVEL
}

vrrp_instance VI_1 {
    state MASTER	#node4  BACKUP
    interface ens33	#修改成主机的网卡名
    virtual_router_id 51
    priority 100	#node4  50
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.152.100/24 dev ens33  label ens33:2
    }
}

virtual_server 192.168.152.100 80 {
    delay_loop 6
    lb_algo rr
    lb_kind DR
    persistence_timeout 0
    persistence_granularity 255.255.255.0
    protocol TCP

    real_server 192.168.152.133 80 {
        weight 1
        HTTP_GET {
            url {
              path /
                status_code 200
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }
    real_server 192.168.152.134 80 {
        weight 1
        HTTP_GET {
            url {
              path /
                status_code 200
            }
            connect_timeout 3
            nb_get_retry 3
            delay_before_retry 3
        }
    }
}


```

LVS结点的keepalived异常退出后,VIP和协议转发还会继续保留。备用机的keepalived会启动,导致VIP重复暴露,数据包会混乱



# 二 redis

## 一些常识

磁盘	1: 寻址 ms  2:带宽 G/m

内存	1:寻址 ns  2:带宽 很大

秒>毫秒>微秒>纳秒	内存寻址速度比磁盘快10w倍

I/O buffer :成本问题

磁盘与磁道,扇区:一扇区512Byte带来一个成本变大

索引4K操作系统,无论读多少,都是最少4K从磁盘拿

 

数据库表很大,性能是否下降？

如果表有索引,增删改 变慢。

查询速度呢？

1:一个或少量查询依旧很快  2:并发大的时候会受磁盘带宽影响速度



### Epoll

Linux重定向:0是标准输入 1是标准输出 2是错误输出

```shell
#查看redis fd文件描述符
ps -ef | grep redis
cd /proc/redis PID/fd
ll

yum install -y man man-apges #第二类是系统调用类
man 2 read
man 2 select
man 7 epoll
man 2 mmap
man 2 sendfile #zero copy 零拷贝
```



## redis 查看帮助

```shell
命令行启动redis客户端
ps -ef | grep redis #查看启动的金进程
redis-cli #默认启动6379端口
redis-cli -h #查看帮助文档
redis-cli -p 6380

redis默认有16个库 启动redis客户端后
select 8 #选择8号库
redis-cli -p 6380 -n 8 #连接时直接进入8号库

help 查看帮助的使用格式
help @string	按tab键可以补齐、切换命令
```



## redis 的string&bitmap

redis中的string可以理解成byte 

- 字符串
- 数值
- 位图

```shell
set k1 hello 
get k1
#适用场景:分布式锁
set k1 ooxx nx #nx 表示不存在时设置
get k1
set k2 hello xx #xx 只有存在时才能设置
get k2 #nil

mset k3 v3 k4 v4
mget k3 k4

set k1 "hello"
append k1 " world"
get k1
"hello world"
GETRANGE k1 0 -1 #redis正向索引从0开始,反向索引从-1开始！！！
"hello world"
SETRANGE k1 6 ssssssss
get k1 
"hello ssssssss"
STRLEN k1
(integer) 14

type k1 描述key的类型
set k2 11
OBJECT encoding k2
INCR k2
get k2 
"12"
DECR k2
DECRBY k2 3
INCRBYFLOAT k2 0.6

OBJECT encoding k1
APPEND k1 ccc
get k1 
"hello ssssssssccc"


二进制安全:只取字节流,一个字符一个字节。Hbase也是二进制安全
FLUSHALL
set k1 hello
STRLEN k1
(integer) 5
set k2 9 
OBJECT encoding k2
STRLEN k2
(integer) 1 
APPEND k2 999
get k2
OBJECT encoding k2 
INCR k2
OBJECT encoding k2
STRLEN k2
(integer) 5

set k3 a
STRLEN k3 #(integer) 1
APPEND k3 中 
STRLEN k3 #(integer) 4
get k3 #"a\xe4\xb8\xad"
存储的数据未编码,不同的数据类型在不同语言占用空间大小是不固定的。

退出redis 
redis-cli --raw #编码集格式化 再次进入
修改编码集后,可能会输出不同的结果

GETSET #Set the string value of a key and return its old value #可以减少io
MSETNX #Set multiple keys to multiple values, only if none of the keys exist #原子性操作,全部设置成功,结果返回成功

位图
二进制位 字节和二进制 下标都是从左向右 0-N
SETBIT #Sets or clears the bit at offset in the string value stored at key
SETBIT k1 1 1
get k1 # "@"
SETBIT k1 7 1
get k1 # "A"
SETBIT k1 9 1
get k1 # "A@"
BITCOUNT #Count set bits in a string
BITPOS #Find first bit set or clear in a string
BITPOS k1 1 0 0 # (integer) 1 #最后两位代表字节下标
BITPOS k1 1 1 1 # (integer) 9 #返回在全量的二级制位的位置
BITOP #Perform bitwise operations between strings

FLUSHALL 
SETBIT k1 1 1
SETBIT k1 7 1
get k1 # "A"
SETBIT k2 1 1
SETBIT k2 1 1
get k2 # "B"
BITOP and andkey k1 k2 #k1 和k2 做按位与运算,结果保存到andkey 中了
get andkey # "@"
BITOP or orkey k1 k2
get orkey # "C"

位图应用场景
统计用户登录天数,且窗口随机 #参考图片

活跃用户筛选 #参考图片
SETBIT 20210729 1 1
SETBIT 20210730 1 1
SETBIT 20210730 7 1
BITOP or destkey 20210729 20210730
BITCOUNT destkey 0 -1
```



## Redis的list、set、hash、sorted_set、skiplist

### list

```shell
插入有序
#结构可以是栈 同向命令
lpush k1 a b c d e f 
lpop k1 # "e" 

#结构可以是队列 反向命令
rpush k2 a b c d e f
lpop k2 # "a"

LRANGE k2 0 -1 #列出list

LINDEX #Get an element from a list by its index
LSET #Set the value of an element in a list by its index
LREM #Remove elements from a list #正数移除正向个数 负数移除反向个数
LINSERT #Insert an element before or after another element in a list

LLEN #Get the length of a list

阻塞的单播队列:先进先出,先阻塞的先收到数据
BLPOP #Remove and get the first element in a list, or block until one is available #阻塞的弹出元素

LTRIM #Trim a list to the specified rang

```



### hash

```shell
设置k v,获取k v
HSET sean name sss
HMSET sean age 18 addr js

hget sean age
HMGET sean name age
HKEYS sean
HVALS sean
HGETALL sean

计算
HINCRBYFLOAT sean age 0.5
HGET sean age
HINCRBYFLOAT sean age -1.5

```

### set

```shell
去重,不维护排序,不维护元素的插入和弹出顺序
FLUSHDB
SADD k1 tom sean peter ooxx tom xxoo #(integer) 5 有去重
SMEMBERS k1

交并差集
SINTER k2 k3 #交集
SINTERSTORE dest k2 k3 # 存储到 dest,没有发生io
SMEMBERS dest 
SUNION k2 k3 #并集
SDIFF #差集 有方向
SDIFF k2 k3
SDIFF k3 k2

随机抽奖
SRANDMEMBER #Get one or multiple random members from a set #详解查看图
SRANDMEMBER k1 5 -5 10 -10 #正数取出一个去重的结果集 负数是可以有重复的

SPOP #Remove and return one or multiple random members from a set

```



### sorted_set

```shell
help @sorted_set
key的元素所代表的某一属性分值进行排序,元素有反向索引
ZADD #Add one or more members to a sorted set, or update its score if it already exists
物理内存左小右大
ZADD k1 8 apple 2 banana 3 orange
ZRANGE k1 0 -1 
ZRANGE k1 0 -1 withscores
ZrevRANGE k1 0 -1 #反向
ZSCORE k1 apple #根据分数
ZRANk k1 apple #根据排名
ZINCRBY k1 2.5 banana # "4.5"

ZUNIONSTORE #Add multiple sorted sets and store the resulting sorted set in a new key #集合操作

```

sorted_set如何实现快速排序？数据是链表

底层的存储结构:skiplist

牺牲一定的存储空间,换取查询的速度

新元素是否添加层是随机的

类平衡树

速度问题:需要数据达到一定量,平均性能是最优的



## Redis消息订阅、pipeline、事务、modules、布隆过滤器、缓存LRU

redis作为 数据库 或者 缓存



### 消息订阅、pipeline、事务

```shell
cd /etc/redis
vim 6379.conf #修改redis配置文件

开启一个redis-cli
在开启一个nc
yum install -y nc
nc localhost 6379 #与本地6379 端口建立连接
keys *
set k1 hello

redis-cli:
get k1

pipeline管道
echo -e "asdfa\nasdfa" # -e识别换行符
echo -e "set k2 99\n incr k2\n get k2" | nc localhost 6379

redis-cli:
get k2
冷启动概念

发布订阅
help @pubsub
PUBLISH sss hello
SUBSCRIBE sss #监听之后,可以收到PUBLISH 发送的消息,监听之前的数据是收不到的
聊天室消息如何存储数据？？？关系型数据库 or redis？？？
可以按时间选择数据存储的位置

redis事务
redis是单进程,一个客户端的事务不会阻塞另一个客户端
事务方面看哪个客户端的exec命令先到达,就先执行谁的
Watch命令 Redis 还可以通过乐观锁(optimistic lock)实现 CAS (check-and-set)操作

help @transactions
MULTI #Mark the start of a transaction block
EXEC #Execute all commands issued after MULTI
WATCH #Watch the given keys to determine execution of the MULTI/EXEC block

MULTI
set k1 aaa
set k2 bbb
exec
可以开启两个客户端,各自开启事务
一个get k1 另一个 del k1 
先 exec del的客户端执行成功,get会失败

```

### 布隆过滤器

RedisBloom:https://github.com/RedisBloom/RedisBloom

```shell
/root/bf
#yum install -y unzip
wget https://github.com/RedisBloom/RedisBloom/archive/refs/tags/v2.2.4.tar.gz
tar -zxvf v2.2.4.tar.gz
cd RedisBloom-2.2.4/
make #编译
cp redisbloom.so /opt/redis/redis5/
cd /opt/redis/redis5

systemctl stop redis_6379
redis-cli -p 6379 #指定端口登录redis客户端

启动
redis-server  --loadmodule  /opt/redis/redis5/redisbloom.so  
解决缓存穿透问题,搜索到缓存和数据库都没有的信息
还是有一定概率穿透,不可能百分百阻挡, 降低到<1%

BF.ADD sss aaa #(integer) 1
BF.EXISTS sss aaa #(integer) 1
BF.EXISTS sss bbb #(integer) 0

拓展布谷鸟过滤器

有新的key,需要写入redis和布隆过滤器,可能出现双写问题！！！
```



### 缓存LRU

redis作为缓存,缓存应该随着访问变化热数据,redis中的数据如何只随着业务变化,只保留热数据？？？内存大小是有限的,也就是瓶颈

- 业务逻辑		key的有效期

- 业务运转		内存有限,随着访问变化,应该淘汰冷数据

  

```shell
配置文件注意选项
vim /etc/redis/6379.conf
maxmemory <bytes>
maxmemory-policy noeviction
lfu 最少使用,次数
lru 最久未使用,时间

1 业务运转		内存有限,随着访问变化,应该淘汰冷数据
allkeys-lru: 尝试回收最少使用的键(LRU),使得新添加的数据有空间存放。
volatile-lru: 尝试回收最少使用的键(LRU),但仅限于在过期集合的键,使得新添加的数据有空间存放。


2 业务逻辑		key的有效期
set k1 aaa ex 10 #设置时间有效期
ttl k1 #查看key过期时间

发生写之后,会剔除过期时间
set k1 aaa
EXPIRE k1 30
set k1 bbb

倒计时,且redis不能延长
EXPIREAT #Set the expiration for a key as a UNIX timestamp

定时

业务逻辑需要程序补全
```

缓存常见四个问题？

- 击穿
- 雪崩
- 穿透
- 缓存一致性(双写)



## Redis的持久化RDB、fork、copyonwrite、AOF、RDB&AOF混合使用

数据持久化	存储层:快照/副本；日志

RDB redis DB:快照/副本

AOF:日志

```shell
Linux系统 管道 |
管道会触发子进程
eg:
num = 0
echo $num
((num++))
echo $num # 1
((num++)) | echo ok
echo $num # 1
echo $$ #父进程
echo $$ | more #父进程
echo $BASHPID #父进程
echo $BASHPID | more #子进程 PID
$$ 优先级高于 |
父进程的数据子进程是否可以看到？？？
父进程的数据可以让子进程看到
子进程的修改不会破坏父进程
父进程的修改不会破坏子进程

num=1
echo $$
/bin/bash #进入一个子进程
yum -y install psmisc #pstree命令
export num
/bin/bash #再次进入子进程
echo num #可以取出 父进程num数据

./test.sh & #后台运行脚本

开创子进程的速度和内存空间够不够？？？
fork子进程写数据到数据库中
man 2 fork #fork - create a child process
fork 速度快,空间小
Linux,  fork() is implemented using copy-on-write pages
copyonwrite #写时复制 创建子进程并不发生复制 

RDB命令:
save
bgsave --> fork创建子进程
配置文件中给出bgsave的规则,配置文件使用的save标识

什么时候使用save？？？场景明确:关机维护

redis配置文件选项
save <seconds> <changes> #SNAPSHOTTING
save 900 1
save 300 10
save 60 10000

```



RDB的弊端:只有一个dump.rdb；丢失数据相对多一些

RDB的优点:数据恢复速度相对较快



AOF append only file:redis的写操作记录到文件中

AOF 丢失数据少,日志文件大,恢复慢

```
RDB+AOF恢复数据:
RDB和AOF 可以同时开启,但是恢复只会使用AOF 
redis是内存数据库-->写操作会触发io
可以调整写io三个级别:no；always;；everysec

redis配置文件相关:
appendonly yes #no
appendfilename "appendonly.aof"

# appendfsync always
appendfsync everysec 
# appendfsync no

no-appendfsync-on-rewrite no #数据敏感性

aof-use-rdb-preamble yes #数据恢复模式


/var/lib/redis/6379 #redis日志文件位置
redis-check-rdb dump.rdb #检查文件

演示:
daemonize no 关闭后台运行 
aof-use-rdb-preamble no 关闭混合模式
redis-server /etc/redis/6379.conf #重新启动redis-server
set k1 1
set k1 2
set k1 3
BGREWRITEAOF #redis重写命令,执行后产生dump.rdb 文件
查看日志文件

auto-aof-rewrite-percentage 100	
auto-aof-rewrite-min-size 64mb	自动重写配置
```



## Redis的集群:主从复制、CAP、PAXOS、cluster分片集群

redis单机问题:单点故障；容量有限；连接数压力

### AKF

```
AKF:微服务业务拆分四个原则的第一项
X轴:全量,镜像
多个redis 主备 读写分离 可用性 解决单点故障方案
Y轴:业务,功能 高内聚 低耦合 性能提高
解决容量有限方案(MySQL的分库)
Z轴:优先级,逻辑再拆分 sharding 通过代理实现分治

主备-->数据一致性问题？
强一致性:同步阻塞,会破坏可用性
弱一致性:会丢失数据
最终一致性:最终数据会一致

高可用:对主做高可用,服务对外表现一致

CAP原理:一致性,可用性,分区容错性
无法三个同时实现
```

Redis使用默认的异步复制,其特点是低延迟和高性能



### 主从复制

```shell
主从复制
演示:一台虚拟机准备三个redis
mkdir /root/test
cd /root/test
cd /root/redis-5.0.12/utils
./install_server.sh #再添加一个6381
systemctl stop redis_6381

cd /root/test
cp /etc/redis/* ./ #拷贝配置文件副本到test文件夹下
修改部分:前台阻塞运行,没有AOF
vim 6379.conf
	daemonize no #yes
	#logfile /var/log/redis_6379.log
	appendonly no

cd /var/lib/redis/
rm -rf ./*
mkdir 6379 6380 6381
redis-server /root/test/6379.conf
redis-server /root/test/6380.conf
redis-server /root/test/6381.conf

三个redis全部启动后,将6379设置成主
redis-cli -p 6379
redis-cli -p 6380
redis-cli -p 6381
6380的redis-cli执行
	REPLICAOF 127.0.0.1 6379

6379 set k1 aaa
6379 get k1 
6380 get k1 #成功
6380 set k2 bbb #失败

6381和6380的redis-cli执行
	REPLICAOF 127.0.0.1 6379
6381 set k2 ddd
追随6379成功后 老的数据会删除

情景:6381挂了
重新启动6381后追随6379,数据是增量同步还是从头开始？？？
redis-server /root/test/6381.conf --replicaof 127.0.0.1 6379

redis-server /root/test/6381.conf --replicaof 127.0.0.1 6379 --appendonly yes #会再次落RDB

如果主6379挂了
6380的redis-cli执行
	REPLICAOF no one #不在追随6379
6381的redis-cli执行
	REPLICAOF 127.0.0.1 6380
	
配置文件:主从复制
replica-serve-stale-data yes #是否同步完再传输数据
replica-read-only yes #备机是否支持写入
# repl-backlog-size 1mb #增量复制 超过设置大小 需要全量复制同步
# min-replicas-to-write 3 #最少给几个从写成功
# min-replicas-max-lag 10 #

问题！！！
需要人工维护
```



### 哨兵 Sentinel

```shell
哨兵 Sentinel (HA)
Redis Sentinel是Redis官方的高可用性解决方案
    监控(Monitoring)
    提醒(Notification)
    自动故障迁移(Automatic failover)

演示:http://redis.cn/topics/sentinel.html
cd /root/test
vim 26379.conf
	port 26379
	sentinel monitor mymaster 127.0.0.1 6379 2

cp 26379.conf 26380.conf
	port 26380
	sentinel monitor mymaster 127.0.0.1 6379 2
cp 26379.conf 26381.conf
	port 26381
	sentinel monitor mymaster 127.0.0.1 6379 2

redis-server /root/test/6379.conf
redis-server /root/test/6380.conf --replicaof 127.0.0.1 6379
redis-server /root/test/6381.conf --replicaof 127.0.0.1 6379

/root/redis-5.0.12/src #sentinel
/opt/111/redis6/bin #sentinel
redis-server  /root/test/26379.conf --sentinel
通过发布订阅 知道主对应的slave
redis-server  /root/test/26380.conf --sentinel #启动后可以发现其他哨兵
redis-server  /root/test/26381.conf --sentinel
6379主挂了后,sentinel会重新选出新的主
sentinel会修改26379.conf等配置文件

redis-cli -p 6381
	PSUBSCRIBE * #可以查看sentinel之间通信
sentinel的配置文件:
/root/redis-5.0.12/sentinel.conf

```



### cluster 集群

```shell
容量问题:数据没办法划分拆解,sharding分片
	算法:hash table 扩展性差
	逻辑:random
	一致性hash 没有取模:hash映射算法。优点:可以解决其他结点压力；缺点:小部分数据无法命中
弊端:3个模式不能做数据库用

客户端与redis中间添加一层代理层
可以使用nginx,无状态！！！

cluster 集群
无主模型:没有主
每个redis都知道其他的redis,并且都存储了对应的数据分片槽位信息
Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽,

分治问题:聚合操作很难实现事务,数据可能不在一个redis
hash tag

客户端分区就是在客户端就已经决定数据会被存储到哪个redis节点或者从哪个redis节点读取。大多数客户端已经实现了客户端分区。
代理分区 意味着客户端将请求发送给代理,然后代理决定去哪个节点写数据或者读数据。代理根据分区规则决定请求哪些Redis实例,然后根据Redis的响应结果返回给客户端。redis和memcached的一种代理实现就是Twemproxy
查询路由(Query routing) 的意思是客户端随机地请求任意一个redis实例,然后由Redis将请求转发给正确的Redis节点。Redis Cluster实现了一种混合形式的查询路由,但并不是直接将请求从一个redis节点转发到另一个redis节点,而是在客户端的帮助下直接redirected到正确的redis节点

演示:
一 使用twemproxy
https://github.com/twitter/twemproxy #一致性hash代理
mkdir /root/twemproxy
cd /root/twemproxy
git clone https://github.com/twitter/twemproxy.git #下载源码
# yum update nss #如果有报错
cd /root/twemproxy/twemproxy
yum install -y automake  libtool #需要先安装
autoreconf -fvi #顺序执行
./configure --enable-debug=full
make
src/nutcracker -h

cd /root/twemproxy/twemproxy/scripts
cp  nutcracker.init  /etc/init.d/twemproxy #拷贝并修改名称
cd /etc/init.d
chmod +x twemproxy

vim nutcracker.init #查看 按条件创建标签页
mkdir  /etc/nutcracker
cd /root/twemproxy/twemproxy/conf
cp ./* /etc/nutcracker/
cd /etc/nutcracker/ #查看复制后的信息

cd /root/twemproxy/twemproxy/src
cp nutcracker /usr/bin #程序和脚本放到对应的位置 其他位置可以执行

cd /etc/nutcracker
cp nutcracker.yml nutcracker.yml.bak #备份配置文件
vim nutcracker.yml #修改配置文件
	alpha:
  listen: 127.0.0.1:22121
  hash: fnv1a_64
  distribution: ketama
  auto_eject_hosts: true
  redis: true
  server_retry_timeout: 2000
  server_failure_limit: 1
  servers:
   - 127.0.0.1:6379:1
   - 127.0.0.1:6380:1

mkdir -p /root/data/6379
redis-server --port 6379
mkdir -p /root/data/6380 #redis-server当前目录启动,就是当持久化目录
redis-server --port 6380

cd /etc/init.d #准备启动服务
./twemproxy start
redis-cli -p 22121
set XXX #放入数据后  单独连接6379和6380查看的数据是不同的

22121端口不支持keys * 或者watch 等事务操作

二 :使用predixy
https://github.com/joyieldInc/predixy
wget https://github.com/joyieldInc/predixy/releases/download/1.0.5/predixy-1.0.5-bin-amd64-linux.tar.gz

vim 命令行模式
光标位置 --> shift+: --> .,$s/#//
. 表示光标位置 $ 表示最后一行 s 表示替换  将#替换成空

三:redis创建cluster
cd /root/redis-5.0.12/utils/create-cluster
vim README #创建的步骤 
vim create-cluster #查看集群配置
	NODES=6 #六个节点
	REPLICAS=1 #三个主 三个从
./create-cluster start #开启redis实例
	Starting 30001
    Starting 30002
    Starting 30003
    Starting 30004
    Starting 30005
    Starting 30006
./create-cluster create #输出提示
    >>> Performing hash slots allocation on 6 nodes...
    Master[0] -> Slots 0 - 5460
    Master[1] -> Slots 5461 - 10922
    Master[2] -> Slots 10923 - 16383
    Adding replica 127.0.0.1:30005 to 127.0.0.1:30001
    Adding replica 127.0.0.1:30006 to 127.0.0.1:30002
    Adding replica 127.0.0.1:30004 to 127.0.0.1:30003

redis-cli -p 30001 #进入客户端无法set k1 sss
redis-cli -c  -p 30001 #进入正确客户端
开启事务需要再正确的节点上

正确开启事务方法 #添加{oo} 类似分组
    set {oo}k1 aaa
    WATCH {oo}k1
    MULTI
    set {oo}k1 sss
    exec
    get {oo}k1

./create-cluster stop #关闭集群
./create-cluster clean #清理

重新启动,手动分配槽位！！！
./create-cluster start
redis-cli --cluster help #查看帮助
./create-cluster start
redis-cli --cluster create 127.0.0.1:30001 127.0.0.1:30002 127.0.0.1:30003 127.0.0.1:30004 127.0.0.1:30005 127.0.0.1:30006  --cluster-replicas 1

redis-cli -c  -p 30001 #进入客户端

演示 reshard！！！
redis-cli --cluster  reshard  127.0.0.1:30001
	输入移动数量 
	输入移入节点的ID:7b371134ef1c83a2e4e950b52c862298a3d23663
	输入移出节点的ID:2ad1e58c2b88dab125125f4af11ad0fa3835414e
	完成:done
redis-cli --cluster info 127.0.0.1:30001 #查看移动后的信息
redis-cli --cluster check  127.0.0.1:30001

无法精准移动槽位

备注:多台机器纯手工搭建cluster 需要redis启动时添加参数 enable (参考官网)
```



## Redis 击穿、穿透、雪崩、分布式锁

### 击穿、穿透、雪崩

```
redis常用做缓存
	key有过期时间
	lru、lfu淘汰了部分数据
	
击穿:客户端访问的数据不在缓存中,访问直接透传到数据库
	设置锁 1 get key 2 setnx 3-1 ok 去DB 3-2 false sleep
	dead lock:设置锁的过期时间
	锁超时了:多线程 一个线程取DB 一个线程监控是否取回,更新锁的时间
	
雪崩:客户端访问的大量数据不在缓存中,大量访问直接透传到数据库
	时点性无关:随机过期时间 
	零点 强依赖击穿方案 零点延时
	
穿透:客户端访问到数据库不存再的数据 
	布隆过滤器:
		1 放到客户端 2 客户端只包含算法 bitmap-->redis无状态 3 redis集成布隆
	布隆过滤器缺点:只能增加,不能删除(换其他,比如布谷鸟)



```

架构师需要层层限流,并且全局考虑成本等问题,不要因为技术而技术！！！



### 分布式锁

```
setnx设置锁
锁的过期时间
多线程(守护线程)延长过期时间
```

zookeeper:分布式锁,准确性 一致性



# 三 Zookeeper

## Zookeeper介绍、安装、shell cli 使用,基本概念验证

```shell
zookeeper官网:https://zookeeper.apache.org/
ZooKeeper data is kept in-memory

集群模式分类:主从模式 无主模型	复制集群 分片集群
ZooKeeper是主从复制集群
主的单点故障问题？？？
	出现不可用状态-->不可用状态回复到可用状态应该越快越好


zookeeper是一个目录树结构
zookeeper每个node可以存储1M数据
	持久节点
	序列节点
	临时节点-->session:通过session实现锁,连接断开session也会消失
每个客户端连接到zookeeper都会产生一个session 来代表这个客户端


Sequential Consistency - Updates from a client will be applied in the order that they were sent.

Atomicity - Updates either succeed or fail. No partial results.

Single System Image - A client will see the same view of the service regardless of the server that it connects to. i.e., a client will never see an older view of the system even if the client fails over to a different server with the same session.

Reliability - Once an update has been applied, it will persist from that time forward until a client overwrites the update.

Timeliness - The clients view of the system is guaranteed to be up-to-date within a certain time bound.

```



### Zookeeper集群搭建

```shell
安装3.4.6版本zookeeper,安装包再附录链接中
zookeeper安装:四个node主机
ThinkPad:
node01	192.168.245.152
node02	192.168.245.154
node03	192.168.245.155
node04	192.168.245.156

配置Java环境变量 jps命令 查看是否成功

cd /usr/local
rpm -ivh jdk-8u181-linux-x64.rpm
#yum localinstall -y jdk-8u181-linux-x64.rpm
	export JAVA_HOME=/usr/java/default
    export CLASSPATH=.
    export PATH=$PATH:$JAVA_HOME/bin
scp jdk-8u181-linux-x64.rpm root@192.168.245.154:`pwd`
scp jdk-8u181-linux-x64.rpm root@192.168.245.155:`pwd`
scp jdk-8u181-linux-x64.rpm root@192.168.245.156:`pwd`

mkdir /opt/zk
mv zookeeper-3.4.6.tar.gz /opt/zk/

#wget https://archive.apache.org/dist/zookeeper/zookeeper-3.7.0/apache-zookeeper-3.7.0.tar.gz

tar -xf zookeeper-3.4.6.tar.gz
cd /opt/zk/zookeeper-3.4.6/conf
cp zoo_sample.cfg zoo.cfg
vim zoo.cfg
	dataDir=/var/zookeeper/zk #修改后
server.1=192.168.245.152:2888:3888
server.2=192.168.245.154:2888:3888
server.3=192.168.245.155:2888:3888
server.4=192.168.245.156:2888:3888
    #2888 node业务通信端口
    #3888 node选主通信端口
mkdir -p /var/zookeeper/zk
cd /var/zookeeper/zk
vim myid
	1 #写入当前node 的ID #四个node各不相同
cd /opt #将安装的 zookeeper 分发到其他node
scp -r ./zk root@192.168.245.154:`pwd`
scp -r ./zk root@192.168.245.155:`pwd`
scp -r ./zk root@192.168.245.156:`pwd`

mkdir -p /var/zookeeper/zk
echo 2 > /var/zookeeper/zk/myid

配置zookeeper环境变量
vim /etc/profile
	export ZOOKEEPER_HOME=/opt/zk/zookeeper-3.4.6
	export PATH=$JAVA_HOME/bin:$PATH:$ZOOKEEPER_HOME/bin
source /etc/profile
scp /etc/profile root@192.168.245.154:/etc
scp /etc/profile root@192.168.245.155:/etc
scp /etc/profile root@192.168.245.156:/etc

启动zookeeper
zkServer.sh help #查看启动帮助

zkServer.sh start-foreground #前台运行
zkServer.sh status #查看节点状态 主还是从
zkCli.sh #启动客户端 默认连接自己
	help #查看命令
create /ooxx "" #创建一个 结点
create /ooxx/xxoo ""
set /ooxx "hellp" #放入数据 #二进制安全的
get /ooxx #查看结点数据

```



```shell
leader node将所有事务递增记录
Zxid #64位表示  前32位表示leader纪元,第几任leader   后32位表示事务递增id ！！！

cZxid	#事务id
mZxid	#
pZxid	#

ephemeralOwner #临时拥有者
create -e #创建临时结点 session结束回收
create -s #序列化  区分同一目录下创建数据

如果客户端当前连接的node挂了,重新与其他node建立连接,session会怎么样？？？
session创建会递增 Zxid 
客户端断开也会递增 Zxid 

```

```
总结
1 统一配置管理 1M数据
2 分组管理 path结构
3 统一命名 sequential
4 同步 临时节点 --> 分布式锁 --> 依托一个父节点,且具备-s,代表父节点下可以有多把锁

可以做HA,选主
不要把zookeeper当数据库使用
```



``` 
netstat -natp | egrep '(2888|3888)' #查看端口连接状态

```



## Zookeeper原理知识,paxos、zab、角色功能、API开发基础

### Paxos

```
zookeeper分布式协调
扩展,可靠性,时序性,快速

扩展--> 框架架构--> 角色--> leader/follower/observer
	--> 读写分离 --> 只有follower才能选举
	--> zoo.cfg  server.4=node04:2888:3888:observer 

可靠性--> 快速恢复leader
	--> 数据可靠 可用 一致性--> 最终一致性,过程中 节点是否对外提供服务

```



```
Paxos 默认没有拜占庭将军问题,只有在一个可信的计算环境中才能成立,这个环境是不会被入侵所破坏的。
Paxos:它是一个基于消息传递的一致性算法

过半通过
最终一致性

```



```
参考网站:https://www.douban.com/note/208430424/

Paxos,它是一个基于消息传递的一致性算法,Leslie Lamport在1990年提出,近几年被广泛应用于分布式计算中,Google的Chubby,Apache的Zookeeper都是基于它的理论来实现的,Paxos还被认为是到目前为止唯一的分布式一致性算法,其它的算法都是Paxos的改进或简化。有个问题要提一下,Paxos有一个前提:没有拜占庭将军问题。就是说Paxos只有在一个可信的计算环境中才能成立,这个环境是不会被入侵所破坏的。

关于Paxos的具体描述可以在Wiki中找到:http://zh.wikipedia.org/zh-cn/Paxos算法。网上关于Paxos分析的文章也很多。这里希望用最简单的方式加以描述并建立起Paxos和ZK Server的对应关系。

Paxos描述了这样一个场景,有一个叫做Paxos的小岛(Island)上面住了一批居民,岛上面所有的事情由一些特殊的人决定,他们叫做议员(Senator)。议员的总数(Senator Count)是确定的,不能更改。岛上每次环境事务的变更都需要通过一个提议(Proposal),每个提议都有一个编号(PID),这个编号是一直增长的,不能倒退。每个提议都需要超过半数((Senator Count)/2 +1)的议员同意才能生效。每个议员只会同意大于当前编号的提议,包括已生效的和未生效的。如果议员收到小于等于当前编号的提议,他会拒绝,并告知对方:你的提议已经有人提过了。这里的当前编号是每个议员在自己记事本上面记录的编号,他不断更新这个编号。整个议会不能保证所有议员记事本上的编号总是相同的。现在议会有一个目标:保证所有的议员对于提议都能达成一致的看法。

好,现在议会开始运作,所有议员一开始记事本上面记录的编号都是0。有一个议员发了一个提议:将电费设定为1元/度。他首先看了一下记事本,嗯,当前提议编号是0,那么我的这个提议的编号就是1,于是他给所有议员发消息:1号提议,设定电费1元/度。其他议员收到消息以后查了一下记事本,哦,当前提议编号是0,这个提议可接受,于是他记录下这个提议并回复:我接受你的1号提议,同时他在记事本上记录:当前提议编号为1。发起提议的议员收到了超过半数的回复,立即给所有人发通知:1号提议生效！收到的议员会修改他的记事本,将1好提议由记录改成正式的法令,当有人问他电费为多少时,他会查看法令并告诉对方:1元/度。

现在看冲突的解决:假设总共有三个议员S1-S3,S1和S2同时发起了一个提议:1号提议,设定电费。S1想设为1元/度, S2想设为2元/度。结果S3先收到了S1的提议,于是他做了和前面同样的操作。紧接着他又收到了S2的提议,结果他一查记事本,咦,这个提议的编号小于等于我的当前编号1,于是他拒绝了这个提议:对不起,这个提议先前提过了。于是S2的提议被拒绝,S1正式发布了提议: 1号提议生效。S2向S1或者S3打听并更新了1号法令的内容,然后他可以选择继续发起2号提议。

好,我觉得Paxos的精华就这么多内容。现在让我们来对号入座,看看在ZK Server里面Paxos是如何得以贯彻实施的。

小岛(Island)——ZK Server Cluster

议员(Senator)——ZK Server

提议(Proposal)——ZNode Change(Create/Delete/SetData…)

提议编号(PID)——Zxid(ZooKeeper Transaction Id)

正式法令——所有ZNode及其数据

貌似关键的概念都能一一对应上,但是等一下,Paxos岛上的议员应该是人人平等的吧,而ZK Server好像有一个Leader的概念。没错,其实Leader的概念也应该属于Paxos范畴的。如果议员人人平等,在某种情况下会由于提议的冲突而产生一个“活锁”(所谓活锁我的理解是大家都没有死,都在动,但是一直解决不了冲突问题)。Paxos的作者Lamport在他的文章”The Part-Time Parliament“中阐述了这个问题并给出了解决方案——在所有议员中设立一个总统,只有总统有权发出提议,如果议员有自己的提议,必须发给总统并由总统来提出。好,我们又多了一个角色:总统。

总统——ZK Server Leader

又一个问题产生了,总统怎么选出来的？oh, my god! It’s a long story

现在我们假设总统已经选好了,下面看看ZK Server是怎么实施的。

情况一:

屁民甲(Client)到某个议员(ZK Server)那里询问(Get)某条法令的情况(ZNode的数据),议员毫不犹豫的拿出他的记事本(local storage),查阅法令并告诉他结果,同时声明:我的数据不一定是最新的。你想要最新的数据？没问题,等着,等我找总统Sync一下再告诉你。

情况二:

屁民乙(Client)到某个议员(ZK Server)那里要求政府归还欠他的一万元钱,议员让他在办公室等着,自己将问题反映给了总统,总统询问所有议员的意见,多数议员表示欠屁民的钱一定要还,于是总统发表声明,从国库中拿出一万元还债,国库总资产由100万变成99万。屁民乙拿到钱回去了(Client函数返回)。

情况三:

总统突然挂了,议员接二连三的发现联系不上总统,于是各自发表声明,推选新的总统,总统大选期间政府停业,拒绝屁民的请求。
```



### Zab

```
zab:zookeeper 原子广播协议
	作用在可用状态(有leader)
原子:成功或者失败,没有中间状态
广播:分布式多节点的,全部知道

zookeeper的数据状态在内存
用磁盘保存日志

消息是放到队列中,最终数据会一致
follower可以sync同步leader的数据
leader同步信息两阶段:一 写入日志 二 写到内存


场景一 第一次启动集群！！！
情景二 重启集群,挂了后！！！
自己会有myid  Zxid事务ID

新的leader 需要具备什么？
数据最全 Zxid
myid
过半通过的数据才是真数据,你见到的可用的Zxid

zookeeper选举过程！！！
1 3888造成两两通信
2 只要任何人投票 都会触发 准leader发起自己的投票
3 推选制:先比较Zxid 相同再比较myid

```



### Watch

```
zookeeper:
1 统一视图:访问任何node看到的数据是一样的
2 目录树结构
3 Watch

方向性 时效性
心跳:主动访问node是否正常
Watch:zookeeper监控node,node挂了之后触发callback回调
callback需要先注册 监控基于session

```



## Zookeeper案例:分布式配置注册发现、分布式锁、ractive模式编程

### 分布式配置

```
服务器配置变更需要通过watch 获知 --> zookeeper配置数据data

```



### 分布式锁

```
zookeeper分布式锁！！！
1 争抢锁 只有一个人能获得锁
2 获得锁的人出问题,临时节点(session) 
3 获得锁的人成功了,释放锁
4 锁被释放 删除,别人怎么知道
4-1 主动轮询 心跳... 延迟 压力
4-2 watch:callback解决延迟问题,弊端 压力
4-3 序列节点(create -s)+watch:watch谁？？？watch前一个,最小的一个获得锁
一旦最小的释放了锁,zk只给第二个发事件回调
锁的重入概念
```







# 四 Kafka

## 1 Kafka初始,架构模型,角色功能梳理

### Kafka  what why how 

```
分布式:可靠的 可扩展 高性能

AKF:
x 节点单点故障 高可用
y 业务划分 topic
z 分片分治 partition
```

```
两个独立的 普遍的问题
    1 单点问题
    2 性能问题 --> kafka业务区分

生产者生成数据 放入topic的不同分区,消费者如何有序消费？？？
1 无关数据分散到不同的分区,并发并行消费
2 有关的数据,一定要按原有顺序发送到同一个分区里

分区--> X轴 副本 不同的主机之间

offset -->  分区内部数据是有序的,分区外部无序
offset 新版本不需要存放到zookeeper中

broker相当于一个进程,一台主机可以启动多个,但是一般只启动一个
broker 选 leader 会用到zookeeper

```



```

生产数据:数据亲密性,不同的数据打入不同的分区
消费数据:数据可以重复被消费,但是一个消费组只能消费一次,不然会导致数据错误

在consumer 消费的时候:数据丢失；数据重复 问题如何解决？？？
runtime内存里维护了自己的offset(偏移量的进度)
#老版本0.8之前 offset是zookeeper维护(对zookeeper影响很大,不推荐)
#topic自己维护offset
#也可以使用第三方 redis mysql维护offset(不推荐)

围绕offset,也就是消费的进度  节奏？ 频率？ 先后？
    1 异步的 5s之内,先干活,5秒到了持久化offset --> 可能丢失offset,造成重复消费
    2 同步的 业务的操作和offset的持久化
    3 没有控制好顺序,offset持久了,但是业务写失效了 

```



```
topic数据 offset两阶段:内存 到 持久化

Kafka与磁盘和网卡的技术点
	推导出业务逻辑的设计
	顺序IO ZeroCopy(sendfile) DMI(协处理器)

```



## 2 Kafka集群搭建,topic+partition消费逻辑梳理

### Kafka集群搭建

```shell
node配置
ThinkPad:
node01	192.168.245.152
node02	192.168.245.154
node03	192.168.245.155
node04	192.168.245.156

node01 - node04
Kafka:node01-node03
zookeeper:node02-node04


同步时钟
yum install -y ntp
#ntpdate ntp[1-7].aliyun.com  七个服务器
ntpdate ntp1.aliyun.com
clock -w #同步


cd /opt
wget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.8.1/kafka_2.13-2.8.1.tgz --no-check-certificate
#wget https://mirrors.tuna.tsinghua.edu.cn/apache/kafka/2.8.0/kafka_2.13-2.8.0.tgz
scp kafka_2.13-2.8.0.tgz root@192.168.245.152:`pwd`

Kafka配置文件修改
cd /opt/kafka_2.13-2.8.0/config
vim server.properties 
    broker.id=0 1 2 ##Kafka node
    listeners=PLAINTEXT://192.168.245.152:9092 #Kafka node IP
    log.dirs=/var/kafka-logs
zookeeper.connect=192.168.245.154:2181,192.168.245.155:2181,192.168.245.156:2181/kafka #zookeeper node IP

vim /etc/profile
	export KAFKA_HOME=/opt/kafka_2.13-2.8.0
	export PATH=$KAFKA_HOME/bin

kafka启动:
zkServer.sh start #启动zookeeper
kafka-server-start.sh  ./server.properties #简单启动

zkCli.sh #启动一个zookeeper节点
	ls /
	get  /kafka/cluster/id #查看内部

创建topic
cd $KAFKA_HOME/ #新窗口打开node03演示
cd bin/ #执行命令


kafka-topics.sh --zookeeper  192.168.245.154:2181,192.168.245.155:2181/kafka  --create  --topic qqqq  --partitions 2  --replication-factor  2
#192.168.245.155:22181/kafka  指定zk中根目录下的节点

kafka-topics.sh --zookeeper  192.168.245.154:2181,192.168.245.155:2181/kafka  --list
kafka-topics.sh --zookeeper  192.168.245.154:2181,192.168.245.155:2181/kafka  --describe  --topic qqqq #查看

```

### 演示！！！

```shell
演示！！！
node02新开窗口:创建消费者  连接Kafka的node  
cd /opt/kafka_2.13-2.8.0/bin 
kafka-console-consumer.sh #查看帮助
kafka-console-consumer.sh --bootstrap-server  192.168.245.152:9092,192.168.245.154:9092  --topic qqqq  --group msb
# --group msb 消费者归属哪个组
> #接收到不同的信息
kafka-console-consumer.sh --bootstrap-server  192.168.245.152:9092,192.168.245.154:9092  --topic qqqq  --group msb
再次开启一个 --topic qqqq 消费者,消息只有一个 消费者可以接受到
因为两个是同组的 --group msb

node01新开窗口:创建生产者
cd /opt/kafka_2.13-2.8.0/bin 
kafka-console-producer.sh #查看帮助
kafka-console-producer.sh  --broker-list  192.168.245.155:9092  --topic qqqq
>hello01 #消费者收到数据
>he #不可以向上向下翻

消费者是订阅topic

node03新开窗口:
kafka-consumer-groups.sh
kafka-consumer-groups.sh  --bootstrap-server 192.168.245.154:9092 --list
kafka-consumer-groups.sh  --bootstrap-server 192.168.245.154:9092 --describe --group msb

zookeeper客户端查看
ls /kafka/brokers/topics #[qqqq, __consumer_offsets]
get  /kafka/brokers/topics/__consumer_offsets #50个partitions

```

```
分区 --> 消息 --> 有序性,有没有,怎么保证？？？
1 如果没有顺序上的约束:分区是水平扩展
	消息是v
2 一但消息(消息很多,但是消息种类很多),而且需要同一类消息的有序性
    消息是 k v,相同的key 一定去到一个分区里,
    broker会保证 生产者 推送的消息的顺序
	一个分区里可能有不同的key,且不同的key 是交叉的,或者说相同的key 在一个里而分区里没有排列在一起

```



### offset！！！

```
Kafka消费者 消费数据使用的拉取方式！！！
	自主 按需
	拉取粒度？ 批次

消费者拿到数据批次 是单线程还是多线程处理？？？   ！！！
    1 单线程:一条一条处理,按顺序处理的时候,来更新offset,速度较慢
    	事务方式处理
    1-1 多线程,offset如何维护？？？按条,还是按批？？？
		consumer 按批拉取数据
	追求多线程的性能,减少事务,减少对数据库的压力
	2 多线程 流式计算 充分利用线程
	
按批拉取,数据的重复消费,丢失问题如何解决？？？
	批次的头或尾的绝对更新,依赖了事务的反馈
    1 大数据的多线程处理方案:spark
    2 流式计算
访问redis --> 访问DB --> offset
步骤进行拆分,该并行的并行,该独立的独立。落地DB以事务方式(通过redis获取数据,完善每一条记录的内容)

Kafka consumer以什么粒度 更新&持久化 offset？？？
分上面的场景:单线程 多线程

备注:什么情况下多线程的优势发挥到极致:具备隔离性 ！！！
```



## Go连接Kafka 使用API (个人拓展)

```shell
#https://duoke360.com/post/112
bin/kafka-server-start.sh config/server.properties #启动 kafka 服务
bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --topic test-go --bootstrap-server localhost:9092 #创建 topic 分区

bin/kafka-console-producer.sh --topic test-go --bootstrap-server localhost:9092 #启动 kafka 生产者
bin/kafka-console-consumer.sh --topic test-go --from-beginning --bootstrap-server localhost:9092 #启动 kafka 消费者
bin/kafka-console-consumer.sh --topic topic1 --from-beginning --bootstrap-server localhost:9092 #启动 kafka 消费者

```

```go
package main

import (
	"fmt"
	"log"
	"os"
	"time"

	"github.com/Shopify/sarama"
)

var Address = []string{"192.168.152.158:9092"}

func main() {
	syncProducer(Address)
	// aSyncProducer()
}

//同步消息模式
func syncProducer(address []string) {
	// 配置
	config := sarama.NewConfig()
	// 属性设置
	config.Producer.Return.Successes = true
	config.Producer.Timeout = 5 * time.Second
	// 创建生成者
	p, err := sarama.NewSyncProducer(address, config)
	// 判断错误
	if err != nil {
		log.Printf("sarama.NewSyncProducer err, message=%s \n", err)
		return
	}
	// 最后关闭生产者
	defer p.Close()
	// 主题名称
	topic := "topic1"
	// 消息
	srcValue := "sync: this is a message. index=%d"
	// 循环发消息
	for i := 0; i < 10; i++ {
		// 格式化消息
		value := fmt.Sprintf(srcValue, i)
		// 创建消息
		msg := &sarama.ProducerMessage{
			Topic: topic,
			Value: sarama.ByteEncoder(value),
		}
		// 发送消息
		part, offset, err := p.SendMessage(msg)
		if err != nil {
			log.Printf("send message(%s) err=%s \n", value, err)
		} else {
			fmt.Fprintf(os.Stdout, value+"发送成功,partition=%d, offset=%d \n", part, offset)
		}
		// 每隔两秒发送一个消息
		time.Sleep(2 * time.Second)
	}
}

// 异步消息
func aSyncProducer() {

	config := sarama.NewConfig()
	//等待服务器所有副本都保存成功后的响应
	config.Producer.RequiredAcks = sarama.WaitForAll
	//随机向partition发送消息
	config.Producer.Partitioner = sarama.NewRandomPartitioner
	//是否等待成功和失败后的响应,只有上面的RequireAcks设置不是NoReponse这里才有用.
	config.Producer.Return.Successes = true
	config.Producer.Return.Errors = true
	//设置使用的kafka版本,如果低于V0_10_0_0版本,消息中的timestrap没有作用.需要消费和生产同时配置
	//注意,版本设置不对的话,kafka会返回很奇怪的错误,并且无法成功发送消息
	config.Version = sarama.V0_10_0_1

	fmt.Println("start make producer")
	//使用配置,新建一个异步生产者
	producer, e := sarama.NewAsyncProducer([]string{"192.168.18.128:9092"}, config)
	if e != nil {
		fmt.Println(e)
		return
	}
	defer producer.AsyncClose()

	//循环判断哪个通道发送过来数据.
	fmt.Println("start goroutine")
	go func(p sarama.AsyncProducer) {
		for {
			select {
			case <-p.Successes():
				//fmt.Println("offset: ", suc.Offset, "timestamp: ", suc.Timestamp.String(), "partitions: ", suc.Partition)
			case fail := <-p.Errors():
				fmt.Println("err: ", fail.Err)
			}
		}
	}(producer)

	var value string
	for i := 0; ; i++ {
		time.Sleep(500 * time.Millisecond)
		time11 := time.Now()
		value = "this is a message 0606 " + time11.Format("15:04:05")

		// 发送的消息,主题。
		// 注意:这里的msg必须得是新构建的变量,不然你会发现发送过去的消息内容都是一样的,因为批次发送消息的关系。
		msg := &sarama.ProducerMessage{
			Topic: "topic2",
		}

		//将字符串转化为字节数组
		msg.Value = sarama.ByteEncoder(value)
		//fmt.Println(value)

		//使用通道发送
		producer.Input() <- msg
	}
}

```

```go
package main

import (
	"fmt"
	"time"

	"github.com/Shopify/sarama"
	cluster "github.com/bsm/sarama-cluster"
)

var (
	kafkaConsumer *cluster.Consumer
	kafkaBrokers  = []string{"192.168.152.158:9092"}
	kafkaTopic    = "topic1"
	groupId       = "test_1"
)

func init() {
	// 配置
	var err error
	config := cluster.NewConfig()
	config.Consumer.Return.Errors = true
	config.Group.Return.Notifications = true
	config.Consumer.Group.Rebalance.Strategy = sarama.BalanceStrategyRange
	config.Consumer.Offsets.Initial = -2
	config.Consumer.Offsets.CommitInterval = 1 * time.Second
	config.Group.Return.Notifications = true
	// 创建消费者
	kafkaConsumer, err = cluster.NewConsumer(kafkaBrokers, groupId, []string{kafkaTopic}, config)
	if err != nil {
		panic(err.Error())
	}
	if kafkaConsumer == nil {
		panic(fmt.Sprintf("consumer is nil. kafka info -> {brokers:%v, topic: %v, group: %v}", kafkaBrokers, kafkaTopic, groupId))
	}
	fmt.Printf("kafka init success, consumer -> %v, topic -> %v, ", kafkaConsumer, kafkaTopic)
}

func main() {
	for {
		select {
		case msg, ok := <-kafkaConsumer.Messages():
			if ok {
				fmt.Printf("kafka 接收到的消息: %s \n", msg.Value)
				kafkaConsumer.MarkOffset(msg, "")
			} else {
				fmt.Printf("kafka 监听服务失败")
			}
		case err, ok := <-kafkaConsumer.Errors():
			if ok {
				fmt.Printf("consumer error: %v", err)
			}
		case ntf, ok := <-kafkaConsumer.Notifications():
			if ok {
				fmt.Printf("consumer notification: %v", ntf)
			}
		}
	}
}

```



# 附录
Golang API
https://github.com/samuel/go-zookeeper
https://pkg.go.dev/github.com/samuel/go-zookeeper/zk


马士兵GitHub资料网址:
大数据:https://github.com/msbbigdata 
分布式网络等:https://github.com/bjmashibing
算法:https://github.com/algorithmzuo

百度盘链接:https://pan.baidu.com/s/1XmsMWAQ1P_KlxS1Ktxu_2Q 密码:tkj6

